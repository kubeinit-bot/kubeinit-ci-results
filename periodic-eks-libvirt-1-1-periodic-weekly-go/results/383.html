<!DOCTYPE html>
<html lang="en" class="layout-pf">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title></title>
    <link rel="stylesheet" href="../static/css/patternfly.min.css">
    <link rel="stylesheet" href="../static/css/ara.css">
    
    <link rel="shortcut icon" href="../static/images/favicon.ico">
    
    
</head>

<body>
    <div class="pf-c-page">
        <header role="banner" class="pf-c-page__header">
            <div class="pf-c-page__header-brand">
                <a class="pf-c-page__header-brand-link" href="../">
                    <img class="pf-c-brand" src="../static/images/logo.svg" alt="KubeInit job report">
                </a>
            </div>
            <div class="pf-c-page__header-nav">
                <nav class="pf-c-nav" aria-label="Global">
                    <button class="pf-c-nav__scroll-button" aria-label="Scroll left">
                        <i class="fas fa-angle-left" aria-hidden="true"></i>
                    </button>
                    <ul class="pf-c-nav__horizontal-list">
                        <li class="pf-c-nav__item">
                            
                            <a href="../" class="pf-c-nav__link">Playbooks</a>
                            
                        </li>
                        


                        <li class="pf-c-nav__item">
                            <a href="https://docs.kubeinit.com" class="pf-c-nav__link" target="_blank">Docs</a>
                        </li>
                    </ul>
                </nav>
            </div>
        </header>
        <main role="main" class="pf-c-page__main">
            <section class="pf-c-page__main-section pf-m-light">
                



<div class="pf-c-card pf-m-hoverable pf-c-alert pf-m-success pf-m-inline">

    <ul class="pf-c-data-list" role="list">
        <li class="pf-c-data-list__item">
            <div class="pf-c-data-list__item-row">
                <div class="pf-c-data-list__item-control pf-m-fit-content">
                    
<div class="pf-c-alert__icon" title="Playbook completed successfully">
    <i class="fas fa-check-circle" aria-hidden="true"></i>
</div>

                </div>
                <div class="pf-c-data-list__item-content">
                    <div class="pf-c-data-list__cell pf-m-fit-content">
                        <div style="padding-top:1em; white-space:nowrap;">
                            <span title="Date at which the playbook started">
                                <i class="fas fa-play-circle" aria-hidden="true"></i> 23 Apr 2021 04:17:48 +0200
                            </span><br>
                            <span title="Date at which the playbook ended">
                                <i class="fas fa-stop-circle" aria-hidden="true" title="Date at which the playbook ended"></i> 23 Apr 2021 04:58:03 +0200
                            </span>
                        </div>
                    </div>
                    <div class="pf-c-data-list__cell pf-m-fit-content">
                        <div style="padding-top:1em; white-space:nowrap;" title="Duration of the playbook">
                            <i class="fas fa-stopwatch" aria-hidden="true"></i> 00:40:15.01
                        </div>
                    </div>
                    <div class="pf-c-data-list__cell pf-m-fit-content">
                        <div style="padding-top:1em; white-space:nowrap;" title="Ansible version">
                            Ansible 2.10.8
                        </div>
                    </div>
                    <div class="pf-c-data-list__cell pf-m-flex-5">
                        <div style="padding-top:1em;" title="/root/builds/g-yZ6fMM/0/kubeinit/kubeinit/tmp/kubeinit/playbooks/eks.yml">
                            <a href="../playbooks/1.html">
                                ...0/kubeinit/kubeinit/tmp/kubeinit/playbooks/eks.yml
                            </a>
                        </div>
                    </div>
                    <div class="pf-c-data-list__cell pf-m-flex-1">
                        <div style="padding-top:1em;">
                            <a href="../playbooks/1.html#hosts">1 hosts</a>
                        </div>
                    </div>
                    <div class="pf-c-data-list__cell pf-m-flex-1">
                        <div style="padding-top:1em;">
                            <a href="../playbooks/1.html#plays">2 plays</a>
                        </div>
                    </div>
                    <div class="pf-c-data-list__cell pf-m-flex-1">
                        <div style="padding-top:1em;">
                            <a href="../playbooks/1.html#results">412 results</a>
                        </div>
                    </div>
                    <div class="pf-c-data-list__cell pf-m-flex-1">
                        <div style="padding-top:1em;">
                            <a href="../playbooks/1.html#files">34 files</a>
                        </div>
                    </div>
                    <div class="pf-c-data-list__cell pf-m-flex-1">
                        <div style="padding-top:1em;">
                            <a href="../playbooks/1.html#records">0 records</a>
                        </div>
                    </div>
                    <div class="pf-c-data-list__cell pf-m-flex-1">
                        <details id="cli-arguments-details">
                            <summary><a>CLI arguments</a></summary>
                            <table class="pf-c-table pf-m-compact pf-m-grid-md" role="grid" aria-label="cli-arguments" id="cli-arguments">
                                <thead>
                                    <tr role="row">
                                        <th role="columnheader" scope="col">Argument</th>
                                        <th role="columnheader" scope="col">Value</th>
                                    </tr>
                                </thead>
                                <tbody role="rowgroup">
                                    
                                </tbody>
                            </table>
                        </details>
                    </div>
                </div>
            </div>
            
            <div class="pf-c-data-list__item-row">
                <ul class="pf-c-chip-group pf-m-toolbar">
                    <li>
                        <h4 class="pf-c-chip-group__label">Playbook labels</h4>
                        <ul class="pf-c-chip-group">
                            
                            <li class="pf-c-chip" title="remote_user:root">
                                <span class="pf-c-chip__text">remote_user:root</span>
                                
                                
                                <a class="pf-c-button pf-m-plain" type="button" aria-label="Search for this label" disabled>
                                
                                    <i class="fas fa-search" aria-hidden="true"></i>
                                </a>
                            </li>
                            
                            <li class="pf-c-chip" title="check:False">
                                <span class="pf-c-chip__text">check:False</span>
                                
                                
                                <a class="pf-c-button pf-m-plain" type="button" aria-label="Search for this label" disabled>
                                
                                    <i class="fas fa-search" aria-hidden="true"></i>
                                </a>
                            </li>
                            
                            <li class="pf-c-chip" title="tags:all">
                                <span class="pf-c-chip__text">tags:all</span>
                                
                                
                                <a class="pf-c-button pf-m-plain" type="button" aria-label="Search for this label" disabled>
                                
                                    <i class="fas fa-search" aria-hidden="true"></i>
                                </a>
                            </li>
                            
                        </ul>
                    </li>
                </ul>
            </div>
            
        </li>
    </ul>
</div>




<div class="pf-c-card" style="margin: 1em 0;">
    <div class="pf-c-card__header pf-c-title pf-m-md">
        <h1><strong>Details</strong></h1>
        <ul class="pf-c-list">
            <li><strong>Task</strong>: ../../roles/kubeinit_eks : debug</a></li>
            <li><strong>Action</strong>: ansible.builtin.debug</li>
            <li><strong>Path</strong>: <a href="../files/31.html#line-119">/root/builds/g-yZ6fMM/0/kubeinit/kubeinit/tmp/kubeinit/kubeinit/roles/kubeinit_eks/tasks/30_configure_master_nodes.yml:119</a>
            <li><strong>Host</strong>: <a href="../hosts/1.html">hypervisor-01</a></li>
            <li><strong>Status</strong>: ok</li>
            <li><strong>Started</strong>: 23 Apr 2021 04:57:21 +0200</li>
            <li><strong>Ended</strong>: 23 Apr 2021 04:57:21 +0200</li>
            <li><strong>Duration</strong>: 00:00:00.05</li>
        </ul>
    </div>
    <div class="pf-c-card__body">
        <h1><strong>Result</strong></h1>
        <table class="pf-c-table pf-m-grid-md pf-m-compact" role="grid" id="result-details">
            <thead>
                <tr role="row">
                    <th role="columnheader" scope="col" class="pf-m-width-20">Field</th>
                    <th role="columnheader" scope="col" class="pf-m-width-80">Value</th>
                </tr>
            </thead>
            <tbody>
                
                <tr role="row">
                    <td role="cell" id="changed" data-label="changed" class="pf-m-width-20">
                        <a href="#changed">changed</a>
                    </td>
                    <td role="cell" data-label="Value" class="pf-m-width-80">
                        <div class="codehilite"><pre><span></span>False
</pre></div>

                    </td>
                </tr>
                
                <tr role="row">
                    <td role="cell" id="eks_master_kubeadm_master_init_output" data-label="eks_master_kubeadm_master_init_output" class="pf-m-width-20">
                        <a href="#eks_master_kubeadm_master_init_output">eks_master_kubeadm_master_init_output</a>
                    </td>
                    <td role="cell" data-label="Value" class="pf-m-width-80">
                        <div class="codehilite"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;changed&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
    <span class="nt">&quot;cmd&quot;</span><span class="p">:</span> <span class="s2">&quot;set -o pipefail\nkubeadm reset -f || true\n\nkubeadm config images list\nkubeadm config images list --image-repository eks-service-01.ekscluster.kubeinit.local:5000\nkubeadm config images list --kubernetes-version latest\n# kubeadm config images pull --image-repository eks-service-01.ekscluster.kubeinit.local:5000\n\ncat &lt;&lt; EOF &gt; ~/config.yml\napiVersion: kubeadm.k8s.io/v1beta1\nkind: ClusterConfiguration\netcd:\n  local:\n    imageRepository: \&quot;eks-service-01.ekscluster.kubeinit.local:5000\&quot;\n    imageTag: \&quot;v3.4.14-eks-1-18-1\&quot;\n# certificatesDir: \&quot;/etc/pki/ca-trust/source/anchors\&quot;\ncontrolPlaneEndpoint: \&quot;api.ekscluster.kubeinit.local:6443\&quot;\nkubernetesVersion: \&quot;v1.18.9\&quot;\nnetworking:\n  serviceSubnet: \&quot;10.96.0.0/12\&quot;\n  podSubnet: \&quot;10.244.0.0/16\&quot;\nEOF\n\ncd\nkubeadm init  --upload-certs  --config=config.yml\n\n#kubeadm init #    --image-repository eks-service-01.ekscluster.kubeinit.local:5000 #    --kubernetes-version 1.18.9 #    --control-plane-endpoint \&quot;api.ekscluster.kubeinit.local:6443\&quot; #    --upload-certs #    --pod-network-cidr=10.244.0.0/16\n&quot;</span><span class="p">,</span>
    <span class="nt">&quot;delta&quot;</span><span class="p">:</span> <span class="s2">&quot;0:02:50.777988&quot;</span><span class="p">,</span>
    <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="s2">&quot;2021-04-23 02:57:21.707434&quot;</span><span class="p">,</span>
    <span class="nt">&quot;failed&quot;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
    <span class="nt">&quot;rc&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="s2">&quot;2021-04-23 02:54:30.929446&quot;</span><span class="p">,</span>
    <span class="nt">&quot;stderr&quot;</span><span class="p">:</span> <span class="s2">&quot;W0423 02:54:30.984076   63351 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory\nW0423 02:54:30.999004   63351 cleanupnode.go:99] [reset] Failed to evaluate the \&quot;/var/lib/kubelet\&quot; directory. Skipping its unmount and cleanup: lstat /var/lib/kubelet: no such file or directory\nI0423 02:54:35.999274   63371 version.go:252] remote version is much newer: v1.21.0; falling back to: stable-1.18\nW0423 02:54:37.373376   63371 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\nI0423 02:54:38.994876   63387 version.go:252] remote version is much newer: v1.21.0; falling back to: stable-1.18\nW0423 02:54:40.736187   63387 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\nI0423 02:54:42.601156   63398 version.go:252] remote version is much newer: v1.22.0-alpha.0; falling back to: stable-1.18\nW0423 02:54:43.934020   63398 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\nW0423 02:54:43.987525   63413 common.go:77] your configuration file uses a deprecated API spec: \&quot;kubeadm.k8s.io/v1beta1\&quot;. Please use &#39;kubeadm config migrate --old-config old.yaml --new-config new.yaml&#39;, which will write the new, similar spec using a newer API version.\nW0423 02:54:43.990182   63413 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\nW0423 02:56:55.329967   63413 manifests.go:225] the default kube-apiserver authorization-mode is \&quot;Node,RBAC\&quot;; using \&quot;Node,RBAC\&quot;\nW0423 02:56:55.332171   63413 manifests.go:225] the default kube-apiserver authorization-mode is \&quot;Node,RBAC\&quot;; using \&quot;Node,RBAC\&quot;&quot;</span><span class="p">,</span>
    <span class="nt">&quot;stderr_lines&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;W0423 02:54:30.984076   63351 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W0423 02:54:30.999004   63351 cleanupnode.go:99] [reset] Failed to evaluate the \&quot;/var/lib/kubelet\&quot; directory. Skipping its unmount and cleanup: lstat /var/lib/kubelet: no such file or directory&quot;</span><span class="p">,</span>
        <span class="s2">&quot;I0423 02:54:35.999274   63371 version.go:252] remote version is much newer: v1.21.0; falling back to: stable-1.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W0423 02:54:37.373376   63371 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;I0423 02:54:38.994876   63387 version.go:252] remote version is much newer: v1.21.0; falling back to: stable-1.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W0423 02:54:40.736187   63387 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;I0423 02:54:42.601156   63398 version.go:252] remote version is much newer: v1.22.0-alpha.0; falling back to: stable-1.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W0423 02:54:43.934020   63398 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W0423 02:54:43.987525   63413 common.go:77] your configuration file uses a deprecated API spec: \&quot;kubeadm.k8s.io/v1beta1\&quot;. Please use &#39;kubeadm config migrate --old-config old.yaml --new-config new.yaml&#39;, which will write the new, similar spec using a newer API version.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W0423 02:54:43.990182   63413 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W0423 02:56:55.329967   63413 manifests.go:225] the default kube-apiserver authorization-mode is \&quot;Node,RBAC\&quot;; using \&quot;Node,RBAC\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W0423 02:56:55.332171   63413 manifests.go:225] the default kube-apiserver authorization-mode is \&quot;Node,RBAC\&quot;; using \&quot;Node,RBAC\&quot;&quot;</span>
    <span class="p">],</span>
    <span class="nt">&quot;stdout&quot;</span><span class="p">:</span> <span class="s2">&quot;[preflight] Running pre-flight checks\n[reset] No etcd config found. Assuming external etcd\n[reset] Please, manually reset etcd to prevent further issues\n[reset] Stopping the kubelet service\n[reset] Unmounting mounted directories in \&quot;/var/lib/kubelet\&quot;\n[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]\n[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n[reset] Deleting contents of stateful directories: [/var/lib/dockershim /var/run/kubernetes /var/lib/cni]\n\nThe reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\n\nThe reset process does not reset or clean up iptables rules or IPVS tables.\nIf you wish to reset iptables, you must do so manually by using the \&quot;iptables\&quot; command.\n\nIf your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\nto reset your system&#39;s IPVS tables.\n\nThe reset process does not clean your kubeconfig files and you must remove them manually.\nPlease, check the contents of the $HOME/.kube/config file.\nk8s.gcr.io/kube-apiserver:v1.18.18\nk8s.gcr.io/kube-controller-manager:v1.18.18\nk8s.gcr.io/kube-scheduler:v1.18.18\nk8s.gcr.io/kube-proxy:v1.18.18\nk8s.gcr.io/pause:3.2\nk8s.gcr.io/etcd:3.4.3-0\nk8s.gcr.io/coredns:1.6.7\neks-service-01.ekscluster.kubeinit.local:5000/kube-apiserver:v1.18.18\neks-service-01.ekscluster.kubeinit.local:5000/kube-controller-manager:v1.18.18\neks-service-01.ekscluster.kubeinit.local:5000/kube-scheduler:v1.18.18\neks-service-01.ekscluster.kubeinit.local:5000/kube-proxy:v1.18.18\neks-service-01.ekscluster.kubeinit.local:5000/pause:3.2\neks-service-01.ekscluster.kubeinit.local:5000/etcd:3.4.3-0\neks-service-01.ekscluster.kubeinit.local:5000/coredns:1.6.7\nk8s.gcr.io/kube-apiserver:v1.18.18\nk8s.gcr.io/kube-controller-manager:v1.18.18\nk8s.gcr.io/kube-scheduler:v1.18.18\nk8s.gcr.io/kube-proxy:v1.18.18\nk8s.gcr.io/pause:3.2\nk8s.gcr.io/etcd:3.4.3-0\nk8s.gcr.io/coredns:1.6.7\n[init] Using Kubernetes version: v1.18.9\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;\n[kubelet-start] Writing kubelet environment file with flags to file \&quot;/var/lib/kubelet/kubeadm-flags.env\&quot;\n[kubelet-start] Writing kubelet configuration to file \&quot;/var/lib/kubelet/config.yaml\&quot;\n[kubelet-start] Starting the kubelet\n[certs] Using certificateDir folder \&quot;/etc/kubernetes/pki\&quot;\n[certs] Generating \&quot;ca\&quot; certificate and key\n[certs] Generating \&quot;apiserver\&quot; certificate and key\n[certs] apiserver serving cert is signed for DNS names [eks-master-01.ekscluster.kubeinit.local kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local api.ekscluster.kubeinit.local] and IPs [10.96.0.1 10.0.0.1]\n[certs] Generating \&quot;apiserver-kubelet-client\&quot; certificate and key\n[certs] Generating \&quot;front-proxy-ca\&quot; certificate and key\n[certs] Generating \&quot;front-proxy-client\&quot; certificate and key\n[certs] Generating \&quot;etcd/ca\&quot; certificate and key\n[certs] Generating \&quot;etcd/server\&quot; certificate and key\n[certs] etcd/server serving cert is signed for DNS names [eks-master-01.ekscluster.kubeinit.local localhost] and IPs [10.0.0.1 127.0.0.1 ::1]\n[certs] Generating \&quot;etcd/peer\&quot; certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [eks-master-01.ekscluster.kubeinit.local localhost] and IPs [10.0.0.1 127.0.0.1 ::1]\n[certs] Generating \&quot;etcd/healthcheck-client\&quot; certificate and key\n[certs] Generating \&quot;apiserver-etcd-client\&quot; certificate and key\n[certs] Generating \&quot;sa\&quot; key and public key\n[kubeconfig] Using kubeconfig folder \&quot;/etc/kubernetes\&quot;\n[kubeconfig] Writing \&quot;admin.conf\&quot; kubeconfig file\n[kubeconfig] Writing \&quot;kubelet.conf\&quot; kubeconfig file\n[kubeconfig] Writing \&quot;controller-manager.conf\&quot; kubeconfig file\n[kubeconfig] Writing \&quot;scheduler.conf\&quot; kubeconfig file\n[control-plane] Using manifest folder \&quot;/etc/kubernetes/manifests\&quot;\n[control-plane] Creating static Pod manifest for \&quot;kube-apiserver\&quot;\n[control-plane] Creating static Pod manifest for \&quot;kube-controller-manager\&quot;\n[control-plane] Creating static Pod manifest for \&quot;kube-scheduler\&quot;\n[etcd] Creating static Pod manifest for local etcd in \&quot;/etc/kubernetes/manifests\&quot;\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \&quot;/etc/kubernetes/manifests\&quot;. This can take up to 4m0s\n[apiclient] All control plane components are healthy after 24.057096 seconds\n[upload-config] Storing the configuration used in ConfigMap \&quot;kubeadm-config\&quot; in the \&quot;kube-system\&quot; Namespace\n[kubelet] Creating a ConfigMap \&quot;kubelet-config-1.18\&quot; in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Storing the certificates in Secret \&quot;kubeadm-certs\&quot; in the \&quot;kube-system\&quot; Namespace\n[upload-certs] Using certificate key:\nea3aeed6ddab7733a57d6a8dda2e4809188427eaab989d70044ed02811477d1a\n[mark-control-plane] Marking the node eks-master-01.ekscluster.kubeinit.local as control-plane by adding the label \&quot;node-role.kubernetes.io/master=&#39;&#39;\&quot;\n[mark-control-plane] Marking the node eks-master-01.ekscluster.kubeinit.local as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[bootstrap-token] Using token: 545ah5.kafx91oalojqzwfl\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \&quot;cluster-info\&quot; ConfigMap in the \&quot;kube-public\&quot; namespace\n[kubelet-finalize] Updating \&quot;/etc/kubernetes/kubelet.conf\&quot; to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \&quot;kubectl apply -f [podnetwork].yaml\&quot; with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following command on each as root:\n\n  kubeadm join api.ekscluster.kubeinit.local:6443 --token 545ah5.kafx91oalojqzwfl \\\n    --discovery-token-ca-cert-hash sha256:a5f6595edf49ec39af7247d59fd813cfd6f9f1455c40d49273655aa19acef97b \\\n    --control-plane --certificate-key ea3aeed6ddab7733a57d6a8dda2e4809188427eaab989d70044ed02811477d1a\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n\&quot;kubeadm init phase upload-certs --upload-certs\&quot; to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join api.ekscluster.kubeinit.local:6443 --token 545ah5.kafx91oalojqzwfl \\\n    --discovery-token-ca-cert-hash sha256:a5f6595edf49ec39af7247d59fd813cfd6f9f1455c40d49273655aa19acef97b &quot;</span><span class="p">,</span>
    <span class="nt">&quot;stdout_lines&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;[preflight] Running pre-flight checks&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[reset] No etcd config found. Assuming external etcd&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[reset] Please, manually reset etcd to prevent further issues&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[reset] Stopping the kubelet service&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[reset] Unmounting mounted directories in \&quot;/var/lib/kubelet\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[reset] Deleting contents of stateful directories: [/var/lib/dockershim /var/run/kubernetes /var/lib/cni]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The reset process does not reset or clean up iptables rules or IPVS tables.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;If you wish to reset iptables, you must do so manually by using the \&quot;iptables\&quot; command.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;to reset your system&#39;s IPVS tables.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The reset process does not clean your kubeconfig files and you must remove them manually.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Please, check the contents of the $HOME/.kube/config file.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/kube-apiserver:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/kube-controller-manager:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/kube-scheduler:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/kube-proxy:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/pause:3.2&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/etcd:3.4.3-0&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/coredns:1.6.7&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eks-service-01.ekscluster.kubeinit.local:5000/kube-apiserver:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eks-service-01.ekscluster.kubeinit.local:5000/kube-controller-manager:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eks-service-01.ekscluster.kubeinit.local:5000/kube-scheduler:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eks-service-01.ekscluster.kubeinit.local:5000/kube-proxy:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eks-service-01.ekscluster.kubeinit.local:5000/pause:3.2&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eks-service-01.ekscluster.kubeinit.local:5000/etcd:3.4.3-0&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eks-service-01.ekscluster.kubeinit.local:5000/coredns:1.6.7&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/kube-apiserver:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/kube-controller-manager:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/kube-scheduler:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/kube-proxy:v1.18.18&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/pause:3.2&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/etcd:3.4.3-0&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k8s.gcr.io/coredns:1.6.7&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[init] Using Kubernetes version: v1.18.9&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[preflight] Running pre-flight checks&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[preflight] Pulling images required for setting up a Kubernetes cluster&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[preflight] This might take a minute or two, depending on the speed of your internet connection&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[kubelet-start] Writing kubelet environment file with flags to file \&quot;/var/lib/kubelet/kubeadm-flags.env\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[kubelet-start] Writing kubelet configuration to file \&quot;/var/lib/kubelet/config.yaml\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[kubelet-start] Starting the kubelet&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Using certificateDir folder \&quot;/etc/kubernetes/pki\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;ca\&quot; certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;apiserver\&quot; certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] apiserver serving cert is signed for DNS names [eks-master-01.ekscluster.kubeinit.local kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local api.ekscluster.kubeinit.local] and IPs [10.96.0.1 10.0.0.1]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;apiserver-kubelet-client\&quot; certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;front-proxy-ca\&quot; certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;front-proxy-client\&quot; certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;etcd/ca\&quot; certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;etcd/server\&quot; certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] etcd/server serving cert is signed for DNS names [eks-master-01.ekscluster.kubeinit.local localhost] and IPs [10.0.0.1 127.0.0.1 ::1]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;etcd/peer\&quot; certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] etcd/peer serving cert is signed for DNS names [eks-master-01.ekscluster.kubeinit.local localhost] and IPs [10.0.0.1 127.0.0.1 ::1]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;etcd/healthcheck-client\&quot; certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;apiserver-etcd-client\&quot; certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[certs] Generating \&quot;sa\&quot; key and public key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[kubeconfig] Using kubeconfig folder \&quot;/etc/kubernetes\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[kubeconfig] Writing \&quot;admin.conf\&quot; kubeconfig file&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[kubeconfig] Writing \&quot;kubelet.conf\&quot; kubeconfig file&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[kubeconfig] Writing \&quot;controller-manager.conf\&quot; kubeconfig file&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[kubeconfig] Writing \&quot;scheduler.conf\&quot; kubeconfig file&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[control-plane] Using manifest folder \&quot;/etc/kubernetes/manifests\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[control-plane] Creating static Pod manifest for \&quot;kube-apiserver\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[control-plane] Creating static Pod manifest for \&quot;kube-controller-manager\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[control-plane] Creating static Pod manifest for \&quot;kube-scheduler\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[etcd] Creating static Pod manifest for local etcd in \&quot;/etc/kubernetes/manifests\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \&quot;/etc/kubernetes/manifests\&quot;. This can take up to 4m0s&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[apiclient] All control plane components are healthy after 24.057096 seconds&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[upload-config] Storing the configuration used in ConfigMap \&quot;kubeadm-config\&quot; in the \&quot;kube-system\&quot; Namespace&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[kubelet] Creating a ConfigMap \&quot;kubelet-config-1.18\&quot; in namespace kube-system with the configuration for the kubelets in the cluster&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[upload-certs] Storing the certificates in Secret \&quot;kubeadm-certs\&quot; in the \&quot;kube-system\&quot; Namespace&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[upload-certs] Using certificate key:&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ea3aeed6ddab7733a57d6a8dda2e4809188427eaab989d70044ed02811477d1a&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[mark-control-plane] Marking the node eks-master-01.ekscluster.kubeinit.local as control-plane by adding the label \&quot;node-role.kubernetes.io/master=&#39;&#39;\&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[mark-control-plane] Marking the node eks-master-01.ekscluster.kubeinit.local as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[bootstrap-token] Using token: 545ah5.kafx91oalojqzwfl&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[bootstrap-token] Creating the \&quot;cluster-info\&quot; ConfigMap in the \&quot;kube-public\&quot; namespace&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[kubelet-finalize] Updating \&quot;/etc/kubernetes/kubelet.conf\&quot; to point to a rotatable kubelet client certificate and key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[addons] Applied essential addon: CoreDNS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;[addons] Applied essential addon: kube-proxy&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Your Kubernetes control-plane has initialized successfully!&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;To start using your cluster, you need to run the following as a regular user:&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;  mkdir -p $HOME/.kube&quot;</span><span class="p">,</span>
        <span class="s2">&quot;  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;  sudo chown $(id -u):$(id -g) $HOME/.kube/config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;You should now deploy a pod network to the cluster.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Run \&quot;kubectl apply -f [podnetwork].yaml\&quot; with one of the options listed at:&quot;</span><span class="p">,</span>
        <span class="s2">&quot;  https://kubernetes.io/docs/concepts/cluster-administration/addons/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;You can now join any number of the control-plane node running the following command on each as root:&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;  kubeadm join api.ekscluster.kubeinit.local:6443 --token 545ah5.kafx91oalojqzwfl \\&quot;</span><span class="p">,</span>
        <span class="s2">&quot;    --discovery-token-ca-cert-hash sha256:a5f6595edf49ec39af7247d59fd813cfd6f9f1455c40d49273655aa19acef97b \\&quot;</span><span class="p">,</span>
        <span class="s2">&quot;    --control-plane --certificate-key ea3aeed6ddab7733a57d6a8dda2e4809188427eaab989d70044ed02811477d1a&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Please note that the certificate-key gives access to cluster sensitive data, keep it secret!&quot;</span><span class="p">,</span>
        <span class="s2">&quot;As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use&quot;</span><span class="p">,</span>
        <span class="s2">&quot;\&quot;kubeadm init phase upload-certs --upload-certs\&quot; to reload certs afterward.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Then you can join any number of worker nodes by running the following on each as root:&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;kubeadm join api.ekscluster.kubeinit.local:6443 --token 545ah5.kafx91oalojqzwfl \\&quot;</span><span class="p">,</span>
        <span class="s2">&quot;    --discovery-token-ca-cert-hash sha256:a5f6595edf49ec39af7247d59fd813cfd6f9f1455c40d49273655aa19acef97b &quot;</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>

                    </td>
                </tr>
                
            </tbody>
        </table>
    </div>
</div>

            </section>
            <section class="pf-c-page__main-section">
            </section>
        </main>
    </div>
</body>

</html>